<!DOCTYPE html>
<html lang="en">
<head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# website: http://ogp.me/ns/website#">
	<meta http-equiv="content-type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="description" content="">
	<meta property="og:title" content="An exercise in Solr and DataImportHandler: HathiTrust data">
	<link rel="icon" href="http://robotlibrarian.billdueber.com/" />
	
	
		<title>Robot Librarian | An exercise in Solr and DataImportHandler: HathiTrust data</title>
		<meta property="og:type" content="article">
		<meta property="article:published_time" content= 2009-09-28 >
	
	<meta property="og:description" content="">
	<meta property="og:url" content="http://robotlibrarian.billdueber.com/2009/09/an-exercise-in-solr-and-dataimporthandler-hathitrust-data/">
	<meta property="og:site_name" content="Robot Librarian">
	
	<meta name="generator" content="Hugo 0.24.1" />
   
	
	<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css">

	
	<link rel="stylesheet" href="http://robotlibrarian.billdueber.comcss/style.css">
	<link href="" rel="alternate" type="application/rss+xml" title="Robot Librarian" />
</head>
<nav class="navbar navbar-default navbar-fixed-top visible-xs">
	<div class="container-fluid">
		<div class="navbar-header">
			<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			
				<a class="navbar-brand" href="http://robotlibrarian.billdueber.com">Robot Librarian</a>
			
		</div>
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			<ul class="nav navbar-nav">
				
				
			</ul>
		</div>
	</div>
</nav>
<div class="container-fluid">
	<div class="row">
		<div id="menu" class="hidden-xs col-sm-4 col-md-3">
	<div id="particles-js"></div>
	<div id="menu-content" class="vertical-align">
		<img src="/images/robot_alpha.png" class="center-block" alt="Cheesy picture of robot">
		
			<h1 class="text-center"><a href="http://robotlibrarian.billdueber.com">Robot Librarian</a></h1>
		
		
		
			<small class="text-center center-block">Bill Dueber</small>
		
		
		
		<div id="social" class="text-center">
			<a href="https://twitter.com/billdueber" target="_blank"><i class="fa fa-twitter fa-2x"></i></a>
				
			
			
			
			<a href="https://github.com/https://github.com/billdueber" target="_blank"><i class="fa fa-github-square fa-2x"></i></a>
			
				
			<a href="mailto:bill@dueber.com" target="_blank"><i class="fa fa-envelope fa-2x"></i></a>			
		</div>
		<div id="links" class="text-center">
			
			
		</div>
	</div>
</div>


		<div id="content" class="col-xs-12 col-sm-8 col-md-9">
			<div class="row">
				<div id="post" class="col-sm-offset-1 col-sm-10 col-md-10 col-lg-8">

<main>
	<header>
		<div class="panel-header">
			<h1>An exercise in Solr and DataImportHandler: HathiTrust data
				
			</h1>
			<div class="panel-footer">
				
			</div>
			
		</div>
	</header>
	<br>
	<HR align=center>
	<article>
		

<p>Many of the folks who read this blog (hi, both of you! Mom, say hello to Dad!) are aware, at least tangentially, of the <a href="http://www.hathitrust.org/">HathiTrust</a>. Currently hosted by us at the <a href="http://www.lib.umich.edu/">University of Michigan</a>, the most public interface to its data is a <a href="http://www.vufind.org/">VuFind</a> installation you can access at <a href="http://catalog.hathitrust.org">catalog.hathitrust.org</a> (or, for you smart-phone types, at <a href="http://m.catalog.hathitrust.org/">m.catalog.hathitrust.org</a>). Once you do a metadata search, you get links into the actual page images or a chance to search the fulltext of the selected item (depending on its copyright status).</p>

<p>It&rsquo;s awesome. Seriously. Even in the absence of fulltext, being able to search within an item can be incredibly useful. Give it a shot if you haven&rsquo;t.</p>

<h3 id="you-don-t-always-need-an-opac">You don&rsquo;t always need an OPAC</h3>

<p>But there are plenty of folks who don&rsquo;t want or need a full-flown interface into all the metadata. They&rsquo;ve already got one of those. What they&rsquo;re interested in, mostly, is figuring out how to easily put links in their <em>own</em> OPAC (or whatnot or whoseits) to the HathiTrust if page images or searching are available. See, for example, <a href="http://lens.lib.uchicago.edu/?itemid=library/marc/uc|835086">a typical record from Tod Olson&rsquo;s stuff at U-Chicago</a> &ndash; he sniffs for HathiTrust and Google Books availability via embedded javascript.</p>

<p>To this end, the HathiTrust folks provide a set of <a href="http://www.hathitrust.org/hathifiles">simple, tab-delimited files</a> &ndash; a full extract on the first of every month, and nightly updates every &hellip;er&hellip;night.</p>

<p>You can see from the <a href="http://www.hathitrust.org/hathifiles_metadata/">description of the file</a> that it&rsquo;s very simple. Tab-delimited fields of the HathiTrust ID, right information, and all the golden-oldie standard identifiers &ndash; some of which (ISSNs, ISBNs, etc.) are further comma-delimited in cases where multiple values are available and a field repeats. And a title and <em>enumcron</em> (description of an individual volume, e.g., &ldquo;Sept 2007, vol. 33, issue 4&rdquo;), so you have something useful to display if you need to, and that&rsquo;s 98% of what most folks want.</p>

<h3 id="the-smart-way-to-do-it-rdbms">The smart way to do it: RDBMS</h3>

<p>If you want to query this data quickly and easily, the obvious thing to do is to dump it into a database. One main table for the non-repeated values, and either a few key=&gt;value tables (or, if you&rsquo;re lazy, a single key =&gt; type/value) for the repeated ISBNs/ISSNs/whatnot. A quick mod-perl script to set up some data normalization going in and out and persist the prepared SQL queries and you&rsquo;re set.</p>

<p>It&rsquo;s hard to make an argument <em>against</em> using a database for these data. I mean, c&rsquo;mon. We&rsquo;ve got a well-defined structure. An obvious foreign-key. No full-text searching needed. This is practically <em>designed</em> for a good old-fashioned RDBMS. Plus, I&rsquo;ve done this approximately a zillion times before, so I&rsquo;m good and fast at it. Case closed.</p>

<h3 id="how-i-m-gonna-do-it">How I&rsquo;m gonna do it</h3>

<p>Screw that. What I really wanted to do was start messing around with the <a href="http://wiki.apache.org/solr/DataImportHandler">DataImportHandler</a>(DIH) in Solr.</p>

<p>I can make a weak argument for including the data in a Solr instance. To wit, it&rsquo;ll certainly be fast enough for anything I&rsquo;m gonna throw at it, and (more important to me) it&rsquo;s easy to set up datastore-level indexing and querying filters with <a href="http://robotlibrarian.billdueber.com/easy-solr-types-for-library-data/">built-in facilities</a> and/or <a href="http://robotlibrarian.billdueber.com/building-a-solr-text-filter-for-normalizing-data/">custom code</a>. This allows me to build clients that call it without having to worry about manipulating the input much, if at all.</p>

<p>The list of simple DIH examples is&hellip;well, I never really found any good ones, although I&rsquo;m sure they&rsquo;re out there. The documentation isn&rsquo;t bad, but it&rsquo;s not full of complete examples, and almost all of them have to do with the potential complexities of sucking data out of a database, which is what most people want to do. Not me, I&rsquo;ve got flat files to work with.</p>

<p>Luckily, you can fire up an &ldquo;interactive&rdquo; DIH session where, at the very least, you can try to import a few rows of data and see if things are puking. I didn&rsquo;t find the error reports particularly helpful all the time, but it&rsquo;s about a zillion times better than nothing, I can tell you that much.</p>

<h3 id="the-game-plan">The game plan</h3>

<p>We&rsquo;ll start with the assumption that I&rsquo;ve already managed to load a full dump from some date (run with me here; I&rsquo;ll explain how to do it later). Then what we want to do is the following:</p>

<ol>
<li>Every night, download the nightly additions/changes file and gunzip it.</li>
<li>Hit the DIH handle to import all files that (a) have a filename of the right format, and (b) have a created date after the last time the DIH handle was run.</li>
</ol>

<p>And that&rsquo;s it. Get the new stuff, have DIH figure out what&rsquo;s new, and import it.</p>

<p>The first part is easy enough to do with perl/python/ruby/whatever. I&rsquo;ll leave it as an exercise for all you diligent students.</p>

<h4 id="setting-up-solrconfig-xml">Setting up solrconfig.xml</h4>

<p>This is the easy part. Set up the handler, give it a semi-meaningful name, and
call out to a config file.</p>

<pre><code class="language-xml">
  &lt;requestHandler name=&quot;/hathiimport&quot; class=&quot;org.apache.solr.handler.dataimport.DataImportHandler&quot;&gt;
      &lt;lst name=&quot;defaults&quot;&gt;
        &lt;str name=&quot;config&quot;&gt;hathi-data-config.xml&lt;/str&gt;
      &lt;/lst&gt;
  &lt;/requestHandler&gt;  

</code></pre>

<h4 id="define-some-useful-data-types-in-schema-xml">Define some useful data types in <code>schema.xml</code></h4>

<p>I left pretty much all of the boilerplate in <code>schema.xml</code> and just added a few types to deal with identifiers.</p>

<ul>
<li><em>lowercase</em>: return a single token that&rsquo;s been lowercased. Don&rsquo;t muck with it otherwise.</li>
<li><em>genericID</em>: trim it, lowercase it, ditch everything that&rsquo;s not a number or a letter, and return as a single token.</li>
<li><em>numeric</em>: Ditch everything but the first string of digits, and <em>then</em> ditch any leading zeros. Useful when you know it&rsquo;s gotta be an integer.</li>
<li><em>stdnum</em> Find the first set of digits (optionally followed by an &lsquo;X&rsquo; and potentially interspersed with dashes or dots), strip off the leading zeros,  and return it. Good to extract an ISBN from a string like &ldquo;(alt) 123-45-678X electronic only&rdquo;.</li>
<li><em>lccnnormalizer</em>: Custom code to normalize an LCCN as per <a href="http://www.loc.gov/marc/lccn-namespace.html#syntax">this page at the LoC</a>.</li>
</ul>

<pre><code class="language-xml">
&lt;types&gt;
  &lt;!-- lowercases the entire field value, keeping it as a single token.  --&gt;
&lt;fieldType name=&quot;lowercase&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&gt;
  &lt;analyzer&gt;
    &lt;tokenizer class=&quot;solr.KeywordTokenizerFactory&quot;/&gt;
    &lt;filter class=&quot;solr.LowerCaseFilterFactory&quot; /&gt;
  &lt;/analyzer&gt;
&lt;/fieldType&gt;

&lt;!-- Full string, stripped of \W and lowercased --&gt;
 &lt;fieldType name=&quot;genericID&quot; class=&quot;solr.TextField&quot; sortMissingLast=&quot;true&quot;  omitNorms=&quot;true&quot;&gt;
   &lt;analyzer&gt;
     &lt;tokenizer class=&quot;solr.KeywordTokenizerFactory&quot;/&gt;
     &lt;filter class=&quot;solr.LowerCaseFilterFactory&quot;/&gt;
     &lt;filter class=&quot;solr.TrimFilterFactory&quot;/&gt;
     &lt;filter class=&quot;solr.PatternReplaceFilterFactory&quot;
          pattern=&quot;[^\p{L}\p{N}]&quot; replacement=&quot;&quot;  replace=&quot;all&quot;
     /&gt;
   &lt;/analyzer&gt;
&lt;/fieldType&gt;

  &lt;!-- standard number normalizer - extract sequence of digits, strip leading zeroes --&gt;
&lt;fieldType name=&quot;numeric&quot; class=&quot;solr.TextField&quot; sortMissingLast=&quot;true&quot; omitNorms=&quot;true&quot; &gt;
 &lt;analyzer&gt;
   &lt;tokenizer class=&quot;solr.KeywordTokenizerFactory&quot;/&gt;
   &lt;filter class=&quot;solr.LowerCaseFilterFactory&quot;/&gt;
   &lt;filter class=&quot;solr.TrimFilterFactory&quot;/&gt;
   &lt;filter class=&quot;solr.PatternReplaceFilterFactory&quot;
        pattern=&quot;[^0-9]*([0-9]+)[^0-9]*&quot; replacement=&quot;$1&quot;
   /&gt;
   &lt;filter class=&quot;solr.PatternReplaceFilterFactory&quot;
        pattern=&quot;^0*(.*)&quot; replacement=&quot;$1&quot;
   /&gt;
 &lt;/analyzer&gt;
&lt;/fieldType&gt;


  &lt;!-- Simple type to normalize isbn/issn. Just get first string of digits followed by an optional 'x' --&gt;
&lt;fieldType name=&quot;stdnum&quot; class=&quot;solr.TextField&quot; sortMissingLast=&quot;true&quot; omitNorms=&quot;true&quot; &gt;
 &lt;analyzer&gt;
   &lt;tokenizer class=&quot;solr.KeywordTokenizerFactory&quot;/&gt;
   &lt;filter class=&quot;solr.LowerCaseFilterFactory&quot;/&gt;
   &lt;filter class=&quot;solr.TrimFilterFactory&quot;/&gt;
    &lt;filter class=&quot;solr.PatternReplaceFilterFactory&quot;
        pattern=&quot;^[\s0\-\.]*([\d\.\-]+x?).*$&quot; replacement=&quot;$1&quot;
   /&gt;
   &lt;filter class=&quot;solr.PatternReplaceFilterFactory&quot;
        pattern=&quot;[\-\.]&quot; replacement=&quot;&quot;  replace=&quot;all&quot;
   /&gt;
 &lt;/analyzer&gt;
&lt;/fieldType&gt;

&lt;!-- LCCN normalization on both index and query --&gt;
&lt;fieldType name=&quot;lccnnormalizer&quot; class=&quot;solr.TextField&quot;  omitNorms=&quot;true&quot;&gt;
  &lt;analyzer&gt;
    &lt;tokenizer class=&quot;solr.KeywordTokenizerFactory&quot;/&gt;
    &lt;filter class=&quot;solr.LowerCaseFilterFactory&quot;/&gt;
    &lt;filter class=&quot;solr.TrimFilterFactory&quot;/&gt;
    &lt;filter class=&quot;edu.umich.lib.solr.analysis.LCCNNormalizerFilterFactory&quot;/&gt;
  &lt;/analyzer&gt;
&lt;/fieldType&gt;

&lt;!-- since fields of this type are by default not stored or indexed,
     any data added to them will be ignored outright.  --&gt;
&lt;fieldtype name=&quot;ignored&quot; stored=&quot;false&quot; indexed=&quot;false&quot; multiValued=&quot;true&quot; class=&quot;solr.StrField&quot; /&gt;

&lt;/types&gt;

</code></pre>

<h4 id="add-field-definitions-to-schema-xml">Add field definitions to <code>schema.xml</code></h4>

<p>This is pretty straight-forward: just set it up.</p>

<pre><code class="language-xml">
&lt;field name=&quot;htid&quot;        type=&quot;genericID&quot;          indexed=&quot;true&quot;  stored=&quot;true&quot;  multiValued=&quot;true&quot;/&gt;
&lt;field name=&quot;bibnum&quot;       type=&quot;genericID&quot;         indexed=&quot;true&quot;  stored=&quot;true&quot;/&gt;

&lt;field name=&quot;access&quot;       type=&quot;lowercase&quot;         indexed=&quot;true&quot;  stored=&quot;true&quot;/&gt;

&lt;field name=&quot;rights&quot;       type=&quot;lowercase&quot;         indexed=&quot;true&quot;  stored=&quot;true&quot;/&gt;

&lt;field name=&quot;source&quot;       type=&quot;lowercase&quot;         indexed=&quot;true&quot;  stored=&quot;true&quot;/&gt;
&lt;field name=&quot;sourceid&quot;     type=&quot;genericID&quot;         indexed=&quot;true&quot;  stored=&quot;true&quot;/&gt;

&lt;field name=&quot;lccn&quot;         type=&quot;lccnnormalizer&quot; indexed=&quot;true&quot;  stored=&quot;true&quot;  multiValued=&quot;true&quot;/&gt;
&lt;field name=&quot;oclc&quot;         type=&quot;numeric&quot;        indexed=&quot;true&quot;  stored=&quot;true&quot;  multiValued=&quot;true&quot;/&gt;
&lt;field name=&quot;isbn&quot;         type=&quot;stdnum&quot;         indexed=&quot;true&quot;  stored=&quot;true&quot;  multiValued=&quot;true&quot;/&gt;
&lt;field name=&quot;issn&quot;         type=&quot;stdnum&quot;         indexed=&quot;true&quot;  stored=&quot;true&quot;  multiValued=&quot;true&quot;/&gt;

&lt;field name=&quot;title&quot;        type=&quot;text&quot;         indexed=&quot;true&quot; stored=&quot;true&quot;/&gt;
&lt;field name=&quot;imprint&quot;      type=&quot;text&quot;         indexed=&quot;true&quot; stored=&quot;true&quot;/&gt;
&lt;field name=&quot;enumcron&quot;     type=&quot;text&quot;         indexed=&quot;true&quot; stored=&quot;true&quot;/&gt;

  &lt;!-- Ignore the multivalued, comma-delimieted source strings --&gt;

  &lt;field name=&quot;rawLine&quot;  type=&quot;ignored&quot; indexed=&quot;false&quot; stored=&quot;false&quot;/&gt;
  &lt;field name=&quot;issns&quot;  type=&quot;ignored&quot; indexed=&quot;false&quot; stored=&quot;false&quot;/&gt;
  &lt;field name=&quot;isbns&quot;  type=&quot;ignored&quot; indexed=&quot;false&quot; stored=&quot;false&quot;/&gt;
  &lt;field name=&quot;oclcs&quot;  type=&quot;ignored&quot; indexed=&quot;false&quot; stored=&quot;false&quot;/&gt;
  &lt;field name=&quot;lccns&quot;  type=&quot;ignored&quot; indexed=&quot;false&quot; stored=&quot;false&quot;/&gt;

</code></pre>

<h4 id="hathi-data-config-xml-define-how-dih-is-going-to-work">hathi-data-config.xml &ndash; define how DIH is going to work.</h4>

<p>This, of course, is the meat of the heart of the center of the matter.</p>

<p>I&rsquo;m going to make use of four DIH technologies:</p>

<ul>
<li><strong>FileDataSource</strong>: In DIH, you declare a data source from which you&rsquo;ll be sucking the raw data for manipulation and massaging. I&rsquo;m just using a file, so this is for me. You can, as you might expect, pull in from a URL or (as mentioned) a database via JDBC.</li>
<li><strong>FileListEntityProcessor</strong>: Given a directory and a set of criteria for a file, this will return a list of filenames that match those criteria. The criteria we&rsquo;ll be using are (a) a regexp the filename must match, and (b) a creation date after the last time we ran the process.</li>
<li><strong>LineEntityProcessor</strong>: Once you&rsquo;ve got a data source, you need to stream it in somehow. There are Processors for XML and other formats, but this one just pulls in lines one at a time. The documentation all talks about <code>LineEntityProcessor</code> basically only being useful for pulling in, say, a list of filenames, but since my data is all line-by-line, this is what I&rsquo;m using as my primary record-fetcher. It populates a single field called <code>rawLine</code> for later processing.</li>
<li><strong>RegexTransformer</strong>: Allows you to take a field pulled from the datasource (or already derived from previous processing) and do regexp substitutions, group extraction, or splitting.</li>
</ul>

<p>SO&hellip;I&rsquo;m going to:</p>

<ol>
<li>Set up a <code>FileDataSource</code> to read from files</li>
<li>Use <code>FileListEntityProcessor</code> to get a list of files that match my criteria</li>
<li>Run each through <code>LineEntityProcessor</code> to generate a bunch of <code>rawLine</code>s.</li>
<li>Use the <code>RegexTransformer</code> multiple times to extract the data from the line.</li>
</ol>

<p>[If you never went to look at it, this might be a good time to check out the <a href="http://www.hathitrust.org/hathifiles_metadata/">description of the tab-delimited metadata files</a>.]</p>

<pre><code class="language-xml">
  &lt;dataConfig&gt;
    &lt;dataSource name=&quot;fds&quot; encoding=&quot;UTF-8&quot;  type=&quot;FileDataSource&quot; /&gt;
    &lt;document&gt;
      &lt;!-- Get a list of files from the last time the handler ran --&gt;
      &lt;entity name=&quot;hathifile&quot;
              processor=&quot;FileListEntityProcessor&quot;
              newerThan=&quot;${dataimporter.last_index_time}&quot;
              fileName=&quot;^hathi_upd_.*\.txt$&quot;
              rootEntity=&quot;false&quot;
              baseDir=&quot;/Users/dueberb/Documents/devel/hathi&quot;
      &gt;

        &lt;entity name=&quot;hathiline&quot;
                processor=&quot;LineEntityProcessor&quot;
                url=&quot;${hathifile.fileAbsolutePath}&quot;
                rootEntity=&quot;true&quot;
                dataSource=&quot;fds&quot;
                transformer=&quot;RegexTransformer&quot;
        &gt;

&lt;!-- Big ugly regexp to get all the tab-delimited fields --&gt;
          &lt;field column=&quot;rawLine&quot;
                 regex=&quot;^(.*)\t(.*)\t(.*)\t(.*)\t(.*)\t(.*)\t(.*)\t(.*)\t(.*)\t(.*)\t(.*)\t(.*)\t(.*)$&quot;
                 groupNames=&quot;htid,access,rights,bibnum,enumcron,source,sourceid,oclcs,isbns,issns,lccns,title,imprint&quot;
          /&gt;

&lt;!-- Split the multi-values on comma --&gt;

          &lt;field column=&quot;oclc&quot; splitBy=&quot;,&quot; sourceColName=&quot;oclcs&quot; /&gt;
          &lt;field column=&quot;issn&quot; splitBy=&quot;,&quot; sourceColName=&quot;issns&quot; /&gt;
          &lt;field column=&quot;isbn&quot; splitBy=&quot;,&quot; sourceColName=&quot;isbns&quot; /&gt;
          &lt;field column=&quot;lccn&quot; splitBy=&quot;,&quot; sourceColName=&quot;lccns&quot; /&gt;
        &lt;/entity&gt; &lt;!-- end of hathiline --&gt;

      &lt;/entity&gt; &lt;!-- end of hathifile --&gt;
    &lt;/document&gt;
  &lt;/dataConfig&gt;  

</code></pre>

<h3 id="and-it-doesn-t-work">And&hellip;it doesn&rsquo;t work.</h3>

<p>It <em>almost</em> works. The problem is that my attempt to use the variable <code>${dataimporter.last_index_time}</code> is busted. There&rsquo;s <a href="https://issues.apache.org/jira/browse/SOLR-1473">a ticket to fix it</a> and a patch already provided, so it&rsquo;s only a matter of time before it&rsquo;s not an issue.</p>

<p>For the moment, though, we&rsquo;ll change that line to:</p>

<pre><code class="language-xml">
  &lt;entity name=&quot;hathifile&quot;
          processor=&quot;FileListEntityProcessor&quot;
          newerThan=&quot;'NOW/DAY'&quot;
          fileName=&quot;^hathi_upd_.*\.txt$&quot;
          rootEntity=&quot;false&quot;
          baseDir=&quot;/Users/dueberb/Documents/devel/hathi&quot;
  &gt;

</code></pre>

<p>That says to basically take everything created since midnight and use it. If you have cron scripts set up to run this every day, you&rsquo;ll have no problems.</p>

<h3 id="dealing-with-a-full-extract">Dealing with a full extract</h3>

<p>You&rsquo;ll only have to do this once, of course, but it has to be done. Basically, reproduce the DIH handler with a different name, pulling in the data from a full extract (you could, e.g., just change the <em>filename</em> parameter to accept <code>/^hathi_full_.*\.txt$/</code>). Maybe call it <em>hathifullimport</em> instead of <em>hathiimport</em>.</p>

<h3 id="fire-her-up">Fire her up!</h3>

<p>Once you&rsquo;re ready to go, just hit the right URL:</p>

<pre><code>http:://solrmachine:port/solr/hathifullimport?command=full-import&amp;clean=true

http:://solrmachine:port/solr/hathiimport?command=full-import&amp;clean=false
</code></pre>

<p>The first one will get the initial big, full file; the second will pull in all the nightlies you&rsquo;ve downloaded, gunzipped, and put in the right place (provided, of course, they&rsquo;re dated after the last midnight, or they&rsquo;ve fixed DIH to allow the <em>last_index_time</em> syntax).</p>

<h3 id="next-steps">Next steps?</h3>

<p>Beer or wine. Take your pick.</p>

<p>After that, though, it&rsquo;d be a matter of actually writing the download scripts and setting up cron jobs. And, of course, putting a front-end on it if you want, or massaging the data as they come out to return a nice JSON format for your consumers. That sort of thing.</p>

<h3 id="so-wait-is-this-really-worth-doing">So, wait&hellip;is this really worth doing?</h3>

<p>Maybe. Probably not. It was worth it to me to start thinking about DIH and how I can use it. And it might be worth it to you, if you want to play around with these data in the ways that solr makes easy.</p>

<p>But, like to many things, it&rsquo;s less worth <em>doing</em> that it was worth <em>writing up</em>. I learned a lot.</p>

	</article>
	<br>
	
</main>
	


						<br>
						<section id="footer">
							<div class="text-center center-block">
								<ul class="copyright">
									<li>&copy; 2017 <a target="_blank" href="http://robotlibrarian.billdueber.com"></a> | Powered by <a target="_blank" href="https://gohugo.io/">Gohugo</a> | Theme <a target="_blank" href="https://git.pofilo.fr/pofilo/hugo-fatboy">Fatboy</a></li>
								</ul>
							</div>
						</section>
					</div>
				</div>
			</div>
		</div>
	</div>
  
  
	
  
	<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
	<script src="//cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.2/js/bootstrap.min.js"></script>

	

	

	

	

	
	</body>
</html>





